#!/usr/bin/env python

# set username and password in .netrc

import requests
import json
import sys
import os
import shutil
import urllib3
#urllib3.disable_warnings()
from wfutils import config


class CQ():
    conf = config().conf
    base = conf['url']
    
    def abort_job(self, job_id):
        url = "%s/%s/abort" %(self.base, job_id)
        resp = requests.post(url, verify=False).json()
        return resp

    def meta(self, job_id):
        url = "%s/%s/metadata?expandSubWorkflows=true" %(self.base, job_id)
        resp = requests.post(url).json()
        return resp


    def trav(self, w):
        if 'calls' in w:
            for name in w['calls']:
                for task in w['calls'][name]:
                    if task['executionStatus'] == 'Failed':
                        if 'subWorkflowMetadata' in task:
                            t = self.trav(task['subWorkflowMetadata'])
                            if t:
                                return t
                        else:
                            return task
        else:
            return w

    def diagnose(self, job_id):
        """
        Try to diagnose why a job failed.
        """
        meta = self.meta(job_id)
        failed_task = self.trav(meta)
        if os.path.exists(failed_task['stderr']):
            stderr = open(failed_task['stderr']).read()
        else:
            stderr = None

        if not stderr:
            if 'Unable to start job' in failed_task['failures'][0]['message']:
                msg = "Failed submission"
            else:
                msg = "Missing stderr"
        elif 'jgi.RQCFilterStats.toString' in stderr:
            msg = "Failed rqc bug"
        elif 'No bins generated by metabat' in stderr:
            msg = "MAG binning bug"
        elif 'hqmq_bin.zip' in stderr:
            msg = "MAG binning bug2"
        elif "KeyError: 'scaffold" in stderr:
            msg = "Multiple shortened bug"
        else:
            msg = "unknown"
        print("%s: %s" %(job_id, msg))

    def dump_stderr(self, job_id):
        """
        Dump stderr for a failed task
        """
        meta = self.meta(job_id)
        failed_task = self.trav(meta)
        if os.path.exists(failed_task['stderr']):
            print("Dumping %s" % (failed_task['stderr']))
            stderr = open(failed_task['stderr']).read()
            print(stderr)

    def copy_output(self, job_id, outdir):
        url = "%s/%s/metadata" %(self.base, job_id)
        if not os.path.exists(outdir):
            os.makedirs(outdir)
        resp = requests.get(url, verify=False)
        outs = resp.json()['outputs']
        for k in outs:
            print("Copying " + k)
            fn = outs[k]
            shutil.copy(fn, os.path.join(outdir, os.path.basename(fn)))

    def job_status(self, job_id):
        url = "%s/%s/status" %(self.base, job_id)
        resp = requests.get(url, verify=False).json()
        return resp

    def list_jobs(self, status, type=None, key=None, value=None, labels=False):
            if status == 'All':
                query = "includeSubworkflows=false&additionalQueryResultFields=labels"
            elif key and value:
                query = "label={}:{}&additionalQueryResultFields=labels".format(key, value)
            elif labels:
            #query = "status=Succeeded&additionalQueryResultFields=labels&includeSubworkflows=false"
                query = "status={}&additionalQueryResultFields=labels&includeSubworkflows=false".format(status)
            else:
                query = "status={}".format(status)
            url = "%s/query?%s" % (self.base, query)
            resp = requests.get(url, verify=False)
            d = resp.json()
            for i in d['results']:
                for k in ['parentWorkflowId', 'rootWorkflowId', 'name', 'start']:
                    if k not in i:
                        i[k] = '-'

                if labels:
                    if 'labels' not in i:
                        continue
                    i['proj'] = i['labels'].get('project_id', '-')
                    i['vers'] = i['labels'].get('pipeline_version', '-')
                    i['pipeline'] = i['labels'].get('pipeline', '-')
                    i['activity_id'] = i['labels'].get('activity_id', '-')
                    print("{id:36}  {name:14} {status:10} {proj:15} {vers:9}  {pipeline:16} {activity_id}:14 {start}".format(**i))
                else:
                    print("{id:36}  {name:14} {parentWorkflowId:36}  {rootWorkflowId:36}  {status}   {start}".format(**i))


    def jobs(self, jid, filt):
        url = '{}/{}/metadata'.format(self.base, jid)
        data = {}
        resp = requests.get(url)
    
        d = resp.json()
        for f in d['calls']:
            for s in d['calls'][f]:
                if filt and f.find(filt)<0:
                    continue
                if 'jobId' in s:
                    print("#{} {}".format(f, jid), file=sys.stderr)
                    print(s['jobId'])

    def get_jid(self, jid, status, filt):
    
        url = "{}/query?status={}&additionalQueryResultFields=labels".format(self.base, status)
        data = {}
        resp = requests.get(url, verify=False)
        d = resp.json()
        for i in d['results']:
            if 'parentWorkflowId' not in i:
                i['parentWorkflowId'] = '-'
            if 'rootWorkflowId' not in i:
                i['rootWorkflowId'] = '-'
            if 'name' not in i:
                i['name']=""
            if jid and i['id'] == jid:
                self.jobs(i['id'], filt)
            elif jid and i['rootWorkflowId'] == jid:
                self.jobs(i['id'], filt)
            elif not jid:
                self.jobs(i['id'], filt)


def check_arg(ct):
    if len(sys.argv) < ct:
        print("arg required")
        sys.exit(1)

def usage():
    print("usage: cq <abort|status|running|submitted|all|labels|meta>")

def main():
    cq = CQ()
    states = ['running', 'failed', 'submitted',
              'aborted', 'aborting', 'all', 'succeeded']

    if len(sys.argv) < 2:
        usage()
        sys.exit(1)

    command = sys.argv[1]
    if len(sys.argv) > 2:
        job_id = sys.argv[2]
    if command=='abort':
        check_arg(3)
        print(cq.abort_job(job_id))
    
    elif command=='copy':
        check_arg(4)
        outdir = sys.argv[3]
        cq.copy_output(job_id, outdir)
    elif  command=='status':
        check_arg(3)
        resp = cq.job_status(job_id)
        print(resp) 
    elif  command.startswith('meta'):
        resp = cq.meta(job_id)
        print(json.dumps(resp, indent=4)) 
    elif  command=='graph':
        url = "%s/%s/timing" %(job_id)
        resp = requests.get(url, verify=False)
        with open(sys.argv[3], "w") as f:
            f.write(resp.text)
    elif command=='labels':
        status = 'All'
        if len(sys.argv) > 2:
            status = sys.argv[2]
        cq.list_jobs(status, labels=True)
    elif command in states:
        status = command.capitalize()
        cq.list_jobs(status)
    elif  command=='search_label':
        key = sys.argv[2]
        value = sys.argv[3]
        cq.list_jobs('label', key=key, value=value)
    elif  command.startswith('diag'):
        j = sys.argv[2]
        cq.diagnose(j)
    elif  command.startswith('stderr'):
        j = sys.argv[2]
        cq.dump_stderr(j)
    elif command=='jid':
        filt = None
        jid = None
        status = 'Running'
        if len(sys.argv) > 2 and sys.argv[2]!='-':
            jid = sys.argv[2]
        if len(sys.argv) > 3 and sys.argv[3]!='-':
            filt = sys.argv[3]
        if len(sys.argv) > 4:
            status = sys.argv[4].capitalize()
        cq.get_jid(jid, status, filt)
    else:
        print("{} not recongnized".format(command))
        usage()
        sys.exit(1)


def jprint(obj):
    print(json.dumps(obj, indent=2))


if __name__ == '__main__':
    main()
